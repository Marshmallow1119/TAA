{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48d8e382-d367-4c64-85f7-86b72da47198",
   "metadata": {},
   "source": [
    "Consider the Wine Quality Dataset, and use one the dataset form red or white wine.\n",
    "\n",
    "The purpose of this exercise is to implement Bagging and Boosting strategies.\n",
    "\n",
    "Also, at the end, you should try to understand the impact of feature selection process. Includin all the features or select some of them has impact in the global model performance?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207eed4e-be19-44c7-a355-c9a7f23b07f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Preprocess the Data\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Wine Quality dataset (assuming the CSV file is in the current directory)\n",
    "data = pd.read_csv(\"winequality-white.csv\", sep=\";\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e9e169-893e-49d3-ab8b-f3b483bd82d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea61be5-8f0d-443f-8df2-fd064f31a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some statistics\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411cbdfe-a13f-44a3-91ae-abdee96afc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "\n",
    "# Select only 2 classes of wine quality: 3, and 8 #Imbalance problem\n",
    "# OR\n",
    "# Select only 2 classes of wine quality: 5, and 7 #Balance problem\n",
    "\n",
    "filtered_data = '?'\n",
    "\n",
    "# Display the first few rows of the filtered data\n",
    "print(filtered_data.head())\n",
    "\n",
    "# Separate the target variable in a different vector from the features\n",
    "X = '?'\n",
    "y = '?'\n",
    "\n",
    "\n",
    "# Split into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(  '?'   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2445739f-46c7-4cc7-9ed8-264e7e56a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different cross validation strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b7cee0-91df-4c4a-95a3-80d5ce2a7baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the RandomForestClassifier (Bagging)\n",
    "rf_model = RandomForestClassifier(max_depth=10, criterion=\"entropy\", n_estimators=20, random_state=0)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = '?'\n",
    "\n",
    "# Accuracy\n",
    "accuracy_rf = '?'\n",
    "\n",
    "# F1-Score\n",
    "f1_rf = '?'\n",
    "\n",
    "# Precision\n",
    "precision_rf = '?'\n",
    "\n",
    "# Recall\n",
    "recall_rf = '?'\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_rf = '?'\n",
    "\n",
    "# Print metrics for Random Forest (Bagging)\n",
    "print(f\"Random Forest (Bagging) Performance Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_rf:.4f}\")\n",
    "print(f\"F1-Score: {f1_rf:.4f}\")\n",
    "print(f\"Precision: {precision_rf:.4f}\")\n",
    "print(f\"Recall: {recall_rf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c209dd57-405b-4900-abab-556ce23b37cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Predictions\n",
    "gb_model = GradientBoostingClassifier(loss='log_loss', learning_rate=0.9, n_estimators=100)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_gb = '?'\n",
    "\n",
    "# Accuracy\n",
    "accuracy_gb = '?'\n",
    "\n",
    "# F1-Score\n",
    "f1_gb = '?'\n",
    "\n",
    "# Precision\n",
    "precision_gb = '?'\n",
    "\n",
    "# Recall\n",
    "recall_gb = '?'\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_gb = '?'\n",
    "\n",
    "\n",
    "# Print metrics for Gradient Boosting (Boosting)\n",
    "print(f\"Gradient Boosting (Boosting) Performance Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_gb:.4f}\")\n",
    "print(f\"F1-Score: {f1_gb:.4f}\")\n",
    "print(f\"Precision: {precision_gb:.4f}\")\n",
    "print(f\"Recall: {recall_gb:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6dc69e-3401-4de2-8283-fa93675ecc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confusion Matrix for Random Forest\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(cm_rf, annot=True, fmt=\"d\", cmap='Blues', xticklabels=sorted(y.unique()), yticklabels=sorted(y.unique()))\n",
    "plt.title('Confusion Matrix - Random Forest (Bagging)')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "# Plot Confusion Matrix for Gradient Boosting\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(cm_gb, annot=True, fmt=\"d\", cmap='Blues', xticklabels=sorted(y.unique()), yticklabels=sorted(y.unique()))\n",
    "plt.title('Confusion Matrix - Gradient Boosting (Boosting)')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8d8b3c-a26c-49f7-aa00-3abdce92d957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use of ADABoost\n",
    "# Create AdaBoost model using a DecisionTreeClassifier as the base estimator\n",
    "ada_model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=10), n_estimators=100, learning_rate=1)\n",
    "\n",
    "# Train the model\n",
    "ada_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_ada = '?'\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "# Accuracy\n",
    "accuracy_ada = '?'\n",
    "\n",
    "# F1-Score\n",
    "f1_ada = '?'\n",
    "\n",
    "# Precision\n",
    "precision_ada = '?'\n",
    "\n",
    "# Recall\n",
    "recall_ada = '?'\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_ada = '?'\n",
    "\n",
    "\n",
    "# Print metrics for Gradient Boosting (Boosting)\n",
    "print(f\"ADABoost (Boosting) Performance Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_ada:.4f}\")\n",
    "print(f\"F1-Score: {f1_ada:.4f}\")\n",
    "print(f\"Precision: {precision_ada:.4f}\")\n",
    "print(f\"Recall: {recall_ada:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ada90a9-2231-4d10-a397-53c2403784c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confusion Matrix for ADABoost\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(cm_ada, annot=True, fmt=\"d\", cmap='Blues', xticklabels=sorted(y.unique()), yticklabels=sorted(y.unique()))\n",
    "plt.title('Confusion Matrix - ADABoost (Boosting)')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beee7a7-d114-448c-b2df-c827de0ce948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c022aa0-13a6-4217-851c-e7fa73c8f283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3719b88d-5f0a-40de-a3ee-3ee80cab04f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now, if I do feature selection... Can this action have influence on the model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52e78dc-6012-4b1c-a019-451e26b8751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study the features relation - what can you conclude?\n",
    "sns.pairplot(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b39254b-c1e6-4c4a-a6c0-ee631d1323d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(filtered_data, hue=\"quality\", markers=[\"o\", \"s\"], corner=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e83144a-daa8-4e00-b611-6b6692043c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filter Method: SelectKBest\n",
    "# This method selects the best k features based on univariate statistical tests (e.g., chi-squared, mutual information).\n",
    "# This is a quick way to filter out irrelevant features before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9693fd62-8df2-40ac-afd0-907404fcbfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Apply SelectKBest with chi-squared for feature selection\n",
    "selector = SelectKBest(chi2, k=4)  # Select top k features\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "\n",
    "# Retrieve the selected feature names\n",
    "selected_features_mask = '?'\n",
    "selected_feature_names = '?'\n",
    "\n",
    "# Display the selected features\n",
    "print(\"Selected Features:\")\n",
    "print(selected_feature_names)\n",
    "\n",
    "\n",
    "\n",
    "# Since SelectKBest returns a NumPy array, we need to apply the transformation to the test set using the same selector\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Train AdaBoost model with selected features\n",
    "ada_model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=100, random_state=42)\n",
    "ada_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_ada = '?'\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_ada = '?'\n",
    "f1_ada = '?'\n",
    "\n",
    "print(f\"AdaBoost with Feature Selection - Accuracy: {accuracy_ada:.4f}\")\n",
    "print(f\"AdaBoost with Feature Selection - F1 Score: {f1_ada:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bcda8a-9ac7-4c3b-b8fc-debb4cbae213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dea0faf-166c-429c-933d-69a9f42b406f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf4091c-27ea-480e-b8dc-df51619a9e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Wrapper Method: Recursive Feature Elimination (RFE)\n",
    "#RFE recursively removes the least important features based on the modelâ€™s performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809c6996-a5cb-4c1e-80b3-84fe1ed5a5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "\n",
    "# Initialize the model and RFE\n",
    "model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=100, learning_rate=1)\n",
    "selector = RFE(model, n_features_to_select=5)  # Select the top 5 features\n",
    "\n",
    "# Fit RFE\n",
    "selector = selector.fit(X_train, y_train)\n",
    "\n",
    "# Transform the training and test sets\n",
    "X_train_rfe = selector.transform(X_train)\n",
    "X_test_rfe = selector.transform(X_test)\n",
    "\n",
    "\n",
    "# Retrieve the selected feature names\n",
    "selected_features_mask = selector.get_support()  # Boolean mask of selected features\n",
    "selected_feature_names = X.columns[selected_features_mask]  # Get the names of the selected features\n",
    "\n",
    "# Display the selected features\n",
    "print(\"Selected Features:\")\n",
    "print(selected_feature_names)\n",
    "\n",
    "\n",
    "\n",
    "# Train the AdaBoost model\n",
    "model.fit('?', y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_ada_rfe = '?'\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_ada_rfe = '?'\n",
    "f1_ada_rfe = '?'\n",
    "\n",
    "print(f\"AdaBoost with RFE - Accuracy: {accuracy_ada_rfe:.4f}\")\n",
    "print(f\"AdaBoost with RFE - F1 Score: {f1_ada_rfe:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa7d562-485d-45b2-a1ea-4c37194f0c39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b6cc8d-18ef-47c5-bec1-33f4fd9519d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d80f0be-cf1f-4a9b-8959-4f8dfa4c7308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Embedded Method: Feature Importance from AdaBoost\n",
    "# Since AdaBoost builds a series of decision trees,\n",
    "# we can use the feature importances generated by the model to select the most important features.\n",
    "# This is an embedded method because feature selection happens during the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df27e20b-039a-4038-a637-610213e5311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train AdaBoost model\n",
    "ada_model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=100, learning_rate=1)\n",
    "ada_model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = ada_model.feature_importances_\n",
    "\n",
    "# Sort the features by importance\n",
    "indices = importances.argsort()[::-1]  # Reverse the order so the most important feature is first\n",
    "\n",
    "# Select the top k features (e.g., top 5)\n",
    "top_k = 5 #5\n",
    "X_train_top_k = X_train.iloc[:, indices[:top_k]]\n",
    "X_test_top_k = X_test.iloc[:, indices[:top_k]]\n",
    "\n",
    "\n",
    "# Get feature names\n",
    "feature_names = X.columns\n",
    "# Get the top N most important features (e.g., top 5)\n",
    "\n",
    "top_features = feature_names[indices[:top_k]]\n",
    "\n",
    "# Display the top N selected features based on feature importance\n",
    "print(\"Top 5 selected features based on AdaBoost feature importance:\")\n",
    "print(top_features)\n",
    "\n",
    "\n",
    "\n",
    "# Train and evaluate the model with selected features\n",
    "ada_model.fit(X_train_top_k, y_train)\n",
    "y_pred_ada = '?'\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_ada = '?'\n",
    "f1_ada = '?'\n",
    "\n",
    "print(f\"AdaBoost with Feature Importance - Accuracy: {accuracy_ada:.4f}\")\n",
    "print(f\"AdaBoost with Feature Importance - F1 Score: {f1_ada:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab03f9e-3ead-4a7c-a32d-9889321d218f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c360b6ba-8723-4540-b367-c11cb28e4e75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaff419-5bfd-4545-8faf-408ae9566f68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
